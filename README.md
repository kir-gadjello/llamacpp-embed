# llamacpp-embed
A customized llama.cpp inference runtime I use for my desktop and mobile apps
