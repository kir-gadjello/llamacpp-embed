As a professional C++ engineer, analyze the three source code files I give you. First two files 'rpcserver.h' 'rpcserver.cpp' introduce my own json rpc code for LLM sampling for llama.cpp, the other file 'speculative.cpp' is a speculative decoding example from the llama.cpp repo. Using speculative.cpp as a refernce and template I need you to analyze possible integration issues, plan out step by step, and write a speculative decoding (sampling) API integrated into my json rpc source code.
The speculative decoding api should

Here is an informative summary on speculative decoding with python reference implementation.
```
The article discusses DeepMind's paper on "Accelerating Large Language Model Decoding with Speculative Sampling". The authors propose a method called speculative sampling, which is a technique for decoding text from large language models more efficiently.

The standard method of generating text from a language model is through autoregressive sampling. The algorithm provided in the paper is implemented in the function `autoregressive_sampling(x, model, N)`. Here, `x` is a list of integers representing the token ids of the input text, `model` is a language model like GPT-2, and `N` is the number of tokens we want to decode. The time complexity of this algorithm is proportional to the number of tokens to decode and the time taken for a single forward pass of the model [Source](https://jaykmody.com).

Speculative sampling, on the other hand, uses two models: a smaller, faster draft model and a larger, slower target model. The draft model speculates what the output is steps into the future, while the target model determines how many of those tokens we should accept. The algorithm involves decoding tokens in the regular autoregressive fashion, getting the probability outputs of the target and draft model on the new predicted sequence, and comparing the target and draft model probabilities to determine how many of the tokens we want to keep based on some rejection criteria. If a token is rejected, it is resampled using a combination of the two distributions and no more tokens are accepted. If all tokens are accepted, an additional final token is sampled from the target model probability output [Source](https://jaykmody.com).

The full algorithm for speculative sampling is defined in the paper and implemented in the function `speculative_sampling(x, draft_model, target_model, N, K)`. The time complexity for this algorithm is proportional to the number of iterations in the while loop and the time complexity for each iteration in the loop [Source](https://jaykmody.com).

The authors report empirical speedup numbers for their 70B Chinchilla model using a specially trained 7B Chinchilla as the draft model. The decoding process is reported to be 2 times faster as compared to autoregressive decoding. The empirical speedup numbers were compared to theoretical speedup numbers, which were calculated using the time complexity equations provided in the paper [Source](https://jaykmody.com).

In conclusion, speculative sampling is a technique that can be used with existing models alongside other inference techniques such as quantization, hardware acceleration, flash attention, etc. It requires no changes to the model's architecture, training, or anything like that. It can also be used with top-p/top-k/temperature [Source](https://jaykmody.com).

Python implementation:
def max_fn(x):
    x_max = np.where(x > 0, x, 0)
    return x_max / np.sum(x_max)

def speculative_sampling(x, draft_model, target_model, N, K):
    # NOTE: paper indexes arrays starting from 1, python indexes from 0, so
    # we have to add an extra -1 term when indexing using n, T, or t
    n = len(x)
    T = len(x) + N

    while n < T:
        # Step 1: auto-regressive decode K tokens from draft model and get final p
        x_draft = x
        for _ in range(K):
            p = draft_model(x_draft)
            x_draft = np.append(x_draft, sample(p[-1]))

        # Step 2: target model forward passes on x_draft
        q = target_model(x_draft)

        # Step 3: append draft tokens based on rejection criterion and resample
        # a token on rejection
        all_accepted = True
        for _ in range(K):
            i = n - 1
            j = x_draft[i + 1]
            if np.random.random() < min(1, q[i][j] / p[i][j]):  # accepted
                x = np.append(x, j)
                n += 1
            else:  # rejected
                x = np.append(x, sample(max_fn(q[i] - p[i])))  # resample
                n += 1
                all_accepted = False
                break

        # Step 4: if all draft tokens were accepted, sample a final token
        if all_accepted:
            x = np.append(x, sample(q[-1]))
            n += 1

        # just keeping my sanity
        assert n == len(x), f"{n} {len(x)}"

    return x
```
```C++
/*  rpcserver.h json rpc for llama.cpp inference - my code */
#include <stdbool.h>
#include <stddef.h>
#include <stdint.h>

#ifdef __cplusplus
extern "C" {
#endif

int init(const char *cmd);

void init_async(const char* cmd);

const char* tokenize(const char* req_json);

const char *poll_system_status();

// const char *json_rpc(const char *method, const char *path, const char *headers,
//                      const char *body);

const char* get_completion(const char* req_json);

const char* async_completion_init(const char* req_json);

const char* async_completion_poll(const char* cmd_json);

const char* async_completion_cancel(const char* req_json);

void deinit();

#ifdef __cplusplus
}
#endif
```

```C++
/* rpcserver.cpp json rpc for llama.cpp inference - my code */
#include "build-info.h"
#include "common.h"
#include "grammar-parser.h"
#include "llama.h"
#include "rpcserver.h"

#ifndef NDEBUG
// crash the server in debug mode, otherwise send an http 500 error
#define CPPHTTPLIB_NO_EXCEPTIONS 1
#endif

// #include "httplib.h"
#include "json.hpp"

// auto generated files (update with ./deps.sh)
#include "completion.js.hpp"
#include "index.html.hpp"
#include "index.js.hpp"
#include "json-schema-to-grammar.mjs.hpp"

#include <cstddef>

#ifndef SERVER_VERBOSE
#define SERVER_VERBOSE 1
#endif

// using namespace httplib;
using json = nlohmann::json;

void parse_args(const char* commandLine, int& argc, char**& argv);

struct server_params {
  std::string hostname = "127.0.0.1";
  std::string public_path = "examples/server/public";
  int32_t port = 8080;
  int32_t read_timeout = 600;
  int32_t write_timeout = 600;
};

// completion token output with probabilities
struct completion_token_output {
  struct token_prob {
    llama_token tok;
    float prob;
  };

  std::vector<token_prob> probs;
  llama_token tok;
};

static size_t common_part(const std::vector<llama_token> &a,
                          const std::vector<llama_token> &b) {
  size_t i;
  for (i = 0; i < a.size() && i < b.size() && a[i] == b[i]; i++) {
  }
  return i;
}

enum stop_type {
  STOP_FULL,
  STOP_PARTIAL,
};

static bool ends_with(const std::string &str, const std::string &suffix) {
  return str.size() >= suffix.size() &&
         0 == str.compare(str.size() - suffix.size(), suffix.size(), suffix);
}

static size_t find_partial_stop_string(const std::string &stop,
                                       const std::string &text) {
  if (!text.empty() && !stop.empty()) {
    const char text_last_char = text.back();
    for (int64_t char_index = stop.size() - 1; char_index >= 0; char_index--) {
      if (stop[char_index] == text_last_char) {
        const std::string current_partial = stop.substr(0, char_index + 1);
        if (ends_with(text, current_partial)) {
          return text.size() - char_index - 1;
        }
      }
    }
  }
  return std::string::npos;
}

template <class Iter>
static std::string tokens_to_str(llama_context *ctx, Iter begin, Iter end) {
  std::string ret;
  for (; begin != end; ++begin) {
    ret += llama_token_to_piece(ctx, *begin);
  }
  return ret;
}

static void server_log(const char *level, const char *function, int line,
                       const char *message,
                       const nlohmann::ordered_json &extra) {
  nlohmann::ordered_json log{
      {"timestamp", time(nullptr)}, {"level", level},
      {"function", function},       {"line", line},
      {"message", message},
  };

  if (!extra.empty()) {
    log.merge_patch(extra);
  }

  const std::string str =
      log.dump(-1, ' ', false, json::error_handler_t::replace);
  printf("%.*s\n", (int)str.size(), str.data());
  fflush(stdout);
}

// format incomplete utf-8 multibyte character for output
static std::string tokens_to_output_formatted_string(const llama_context *ctx,
                                                     const llama_token token) {
  std::string out = token == -1 ? "" : llama_token_to_piece(ctx, token);
  // if the size is 1 and first bit is 1, meaning it's a partial character
  //   (size > 1 meaning it's already a known token)
  if (out.size() == 1 && (out[0] & 0x80) == 0x80) {
    std::stringstream ss;
    ss << std::hex << (out[0] & 0xff);
    std::string res(ss.str());
    out = "byte: \\x" + res;
  }
  return out;
}

// convert a vector of completion_token_output to json
static json
probs_vector_to_json(const llama_context *ctx,
                     const std::vector<completion_token_output> &probs) {
  json out = json::array();
  for (const auto &prob : probs) {
    json probs_for_token = json::array();
    for (const auto &p : prob.probs) {
      std::string tok_str = tokens_to_output_formatted_string(ctx, p.tok);
      probs_for_token.push_back(json{
          {"tok_str", tok_str},
          {"prob", p.prob},
      });
    }
    std::string tok_str = tokens_to_output_formatted_string(ctx, prob.tok);
    out.push_back(json{
        {"content", tok_str},
        {"probs", probs_for_token},
    });
  }
  return out;
}

static bool server_verbose = false;

#if SERVER_VERBOSE != 1
#define LOG_VERBOSE(MSG, ...)
#else
#define LOG_VERBOSE(MSG, ...)                                                  \
  do {                                                                         \
    if (server_verbose) {                                                      \
      server_log("VERBOSE", __func__, __LINE__, MSG, __VA_ARGS__);             \
    }                                                                          \
  } while (0)
#endif

#define LOG_ERROR(MSG, ...)                                                    \
  server_log("ERROR", __func__, __LINE__, MSG, __VA_ARGS__)
#define LOG_WARNING(MSG, ...)                                                  \
  server_log("WARNING", __func__, __LINE__, MSG, __VA_ARGS__)
#define LOG_INFO(MSG, ...)                                                     \
  server_log("INFO", __func__, __LINE__, MSG, __VA_ARGS__)

struct llama_server_context {
  bool stream = false;
  bool has_next_token = false;
  std::string generated_text;
  std::vector<completion_token_output> generated_token_probs;

  size_t num_prompt_tokens = 0;
  size_t num_tokens_predicted = 0;
  size_t n_past = 0;
  size_t n_remain = 0;

  json prompt;
  std::vector<llama_token> embd;
  std::vector<llama_token> last_n_tokens;

  llama_model *model = nullptr;
  llama_context *ctx = nullptr;
  gpt_params params;
  llama_sampling_context ctx_sampling;
  int n_ctx;
  int init_success = 0;

  grammar_parser::parse_state parsed_grammar;
  llama_grammar *grammar = nullptr;

  bool truncated = false;
  bool stopped_eos = false;
  bool stopped_word = false;
  bool stopped_limit = false;
  std::string stopping_word;
  int32_t multibyte_pending = 0;

  std::mutex mutex;

  std::unique_lock<std::mutex> lock() {
    return std::unique_lock<std::mutex>(mutex);
  }

  ~llama_server_context() {
    if (ctx) {
      llama_free(ctx);
      ctx = nullptr;
    }
    if (model) {
      llama_free_model(model);
      model = nullptr;
    }
  }

  void rewind() {
    params.antiprompt.clear();
    params.grammar.clear();
    num_prompt_tokens = 0;
    num_tokens_predicted = 0;
    generated_text = "";
    generated_text.reserve(n_ctx);
    generated_token_probs.clear();
    truncated = false;
    stopped_eos = false;
    stopped_word = false;
    stopped_limit = false;
    stopping_word = "";
    multibyte_pending = 0;
    n_remain = 0;
    n_past = 0;

    if (grammar != nullptr) {
      llama_grammar_free(grammar);
      grammar = nullptr;
      ctx_sampling = llama_sampling_context_init(params, NULL);
    }
  }

  bool loadModel(const gpt_params &params_) {
    params = params_;
    std::tie(model, ctx) = llama_init_from_gpt_params(params);
    if (model == nullptr) {
      LOG_ERROR("unable to load model", {{"model", params_.model}});
      return false;
    }
    n_ctx = llama_n_ctx(ctx);
    std::cout << "LOG: llama_n_ctx(ctx): " << n_ctx << "\n";
    last_n_tokens.resize(n_ctx);
    std::fill(last_n_tokens.begin(), last_n_tokens.end(), 0);
    return true;
  }

  std::vector<llama_token> tokenize(const json &json_prompt,
                                    bool add_bos) const {
    // If `add_bos` is true, we only add BOS, when json_prompt is a string,
    // or the first element of the json_prompt array is a string.
    std::vector<llama_token> prompt_tokens;

    if (json_prompt.is_array()) {
      bool first = true;
      for (const auto &p : json_prompt) {
        if (p.is_string()) {
          auto s = p.template get<std::string>();
          std::vector<llama_token> p;
          if (first) {
            p = ::llama_tokenize(ctx, s, add_bos);
            first = false;
          } else {
            p = ::llama_tokenize(ctx, s, false);
          }
          prompt_tokens.insert(prompt_tokens.end(), p.begin(), p.end());
        } else {
          if (first) {
            first = false;
          }
          prompt_tokens.push_back(p.template get<llama_token>());
        }
      }
    } else {
      auto s = json_prompt.template get<std::string>();
      prompt_tokens = ::llama_tokenize(ctx, s, add_bos);
    }

    return prompt_tokens;
  }

  json loadPrompt() {
    auto prompt_tokens = tokenize(prompt, true); // always add BOS

    num_prompt_tokens = prompt_tokens.size();

    if (params.n_keep < 0) {
      params.n_keep = (int)num_prompt_tokens;
    }
    params.n_keep = std::min(n_ctx - 4, params.n_keep);

    // if input prompt is too big, truncate like normal
    if (num_prompt_tokens >= (size_t)n_ctx) {
      const int n_left = (n_ctx - params.n_keep) / 2;
      std::vector<llama_token> new_tokens(
          prompt_tokens.begin(), prompt_tokens.begin() + params.n_keep);
      const int erased_blocks =
          (num_prompt_tokens - params.n_keep - n_left - 1) / n_left;
      new_tokens.insert(new_tokens.end(),
                        prompt_tokens.begin() + params.n_keep +
                            erased_blocks * n_left,
                        prompt_tokens.end());
      std::copy(prompt_tokens.end() - n_ctx, prompt_tokens.end(),
                last_n_tokens.begin());

      LOG_VERBOSE("input truncated",
                  {
                      {"n_ctx", n_ctx},
                      {"n_keep", params.n_keep},
                      {"n_left", n_left},
                      {"new_tokens", tokens_to_str(ctx, new_tokens.cbegin(),
                                                   new_tokens.cend())},
                  });

      truncated = true;
      prompt_tokens = new_tokens;
    } else {
      const size_t ps = num_prompt_tokens;
      std::fill(last_n_tokens.begin(), last_n_tokens.end() - ps, 0);
      std::copy(prompt_tokens.begin(), prompt_tokens.end(),
                last_n_tokens.end() - ps);
    }

    // compare the evaluated prompt with the new prompt
    n_past = common_part(embd, prompt_tokens);

    embd = prompt_tokens;
    if (n_past == num_prompt_tokens) {
      // we have to evaluate at least 1 token to generate logits.
      n_past--;
    }

    // since #3228 we now have to manually manage the KV cache
    llama_kv_cache_seq_rm(ctx, 0, n_past, -1);

    // LOG_VERBOSE("prompt ingested",
                // {
                //     {"n_past", n_past},
                //     {"cached",
                //      tokens_to_str(ctx, embd.cbegin(), embd.cbegin() + n_past)},
                //     {"to_eval",
                //      tokens_to_str(ctx, embd.cbegin() + n_past, embd.cend())},
                // });

    has_next_token = true;

    return json({
        {"n_ctx", n_ctx},
        {"total_prompt_tokens", num_prompt_tokens},
        {"truncated", truncated},
        {"n_past", n_past},
        {"cached",
          tokens_to_str(ctx, embd.cbegin(), embd.cbegin() + n_past)},
        {"to_eval",
          tokens_to_str(ctx, embd.cbegin() + n_past, embd.cend())},
    });
  }

  void beginCompletion() {
    // number of tokens to keep when resetting context
    n_remain = params.n_predict;
    llama_set_rng_seed(ctx, params.seed);
  }

  completion_token_output nextToken() {
    completion_token_output result;
    result.tok = -1;

    if (embd.size() >= (size_t)n_ctx) {
      // Shift context

      const int n_left = n_past - params.n_keep - 1;
      const int n_discard = n_left / 2;

      llama_kv_cache_seq_rm(ctx, 0, params.n_keep + 1,
                            params.n_keep + n_discard + 1);
      llama_kv_cache_seq_shift(ctx, 0, params.n_keep + 1 + n_discard, n_past,
                               -n_discard);

      for (size_t i = params.n_keep + 1 + n_discard; i < embd.size(); i++) {
        embd[i - n_discard] = embd[i];
      }
      embd.resize(embd.size() - n_discard);

      n_past -= n_discard;

      truncated = true;
      LOG_VERBOSE("input truncated", {
                                         {"n_ctx", n_ctx},
                                         {"n_keep", params.n_keep},
                                         {"n_left", n_left},
                                     });
    }

    bool tg = true;
    while (n_past < embd.size()) {
      int n_eval = (int)embd.size() - n_past;
      tg = n_eval == 1;
      if (n_eval > params.n_batch) {
        n_eval = params.n_batch;
      }

      if (llama_decode(ctx,
                       llama_batch_get_one(&embd[n_past], n_eval, n_past, 0))) {
        LOG_ERROR("failed to eval",
                  {
                      {"n_eval", n_eval},
                      {"n_past", n_past},
                      {"embd",
                       tokens_to_str(ctx, embd.cbegin() + n_past, embd.cend())},
                  });
        has_next_token = false;
        return result;
      }
      n_past += n_eval;
    }

    if (params.n_predict == 0) {
      has_next_token = false;
      result.tok = llama_token_eos(ctx);
      return result;
    }

    {
      // out of user input, sample next token
      std::vector<llama_token_data> candidates;
      candidates.reserve(llama_n_vocab(model));

      result.tok = llama_sampling_sample(ctx, NULL, ctx_sampling, last_n_tokens,
                                         candidates);

      llama_token_data_array candidates_p = {candidates.data(),
                                             candidates.size(), false};

      const int32_t n_probs = params.sampling_params.n_probs;
      if (params.sampling_params.temp <= 0 && n_probs > 0) {
        // For llama_sample_token_greedy we need to sort candidates
        llama_sample_softmax(ctx, &candidates_p);
      }

      for (size_t i = 0; i < std::min(candidates_p.size, (size_t)n_probs);
           ++i) {
        result.probs.push_back(
            {candidates_p.data[i].id, candidates_p.data[i].p});
      }

      last_n_tokens.erase(last_n_tokens.begin());
      last_n_tokens.push_back(result.tok);
      if (tg) {
        num_tokens_predicted++;
      }
    }

    // add it to the context
    embd.push_back(result.tok);
    // decrement remaining sampling budget
    --n_remain;

    if (!embd.empty() && embd.back() == llama_token_eos(ctx)) {
      // stopping_word = llama_token_to_piece(ctx, embd.back());
      has_next_token = false;
      stopped_eos = true;
      LOG_VERBOSE("eos token found", {});
      return result;
    }

    has_next_token = params.n_predict == -1 || n_remain != 0;
    return result;
  }

  size_t findStoppingStrings(const std::string &text,
                             const size_t last_token_size,
                             const stop_type type) {
    size_t stop_pos = std::string::npos;
    for (const std::string &word : params.antiprompt) {
      size_t pos;
      if (type == STOP_FULL) {
        const size_t tmp = word.size() + last_token_size;
        const size_t from_pos = text.size() > tmp ? text.size() - tmp : 0;
        pos = text.find(word, from_pos);
      } else {
        pos = find_partial_stop_string(word, text);
      }
      if (pos != std::string::npos &&
          (stop_pos == std::string::npos || pos < stop_pos)) {
        if (type == STOP_FULL) {
          stopping_word = word;
          stopped_word = true;
          has_next_token = false;
        }
        stop_pos = pos;
      }
    }
    return stop_pos;
  }

  completion_token_output doCompletion() {
    auto token_with_probs = nextToken();

    const std::string token_text =
        token_with_probs.tok == -1
            ? ""
            : llama_token_to_piece(ctx, token_with_probs.tok);
    generated_text += token_text;

    if (params.sampling_params.n_probs > 0) {
      generated_token_probs.push_back(token_with_probs);
    }

    if (multibyte_pending > 0) {
      multibyte_pending -= token_text.size();
    } else if (token_text.size() == 1) {
      const char c = token_text[0];
      // 2-byte characters: 110xxxxx 10xxxxxx
      if ((c & 0xE0) == 0xC0) {
        multibyte_pending = 1;
        // 3-byte characters: 1110xxxx 10xxxxxx 10xxxxxx
      } else if ((c & 0xF0) == 0xE0) {
        multibyte_pending = 2;
        // 4-byte characters: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
      } else if ((c & 0xF8) == 0xF0) {
        multibyte_pending = 3;
      } else {
        multibyte_pending = 0;
      }
    }

    if (multibyte_pending > 0 && !has_next_token) {
      has_next_token = true;
      n_remain++;
    }

    if (!has_next_token && n_remain == 0) {
      stopped_limit = true;
    }

    LOG_VERBOSE("next token",
                {
                    {"token", token_with_probs.tok},
                    {"token_text", tokens_to_output_formatted_string(
                                       ctx, token_with_probs.tok)},
                    {"has_next_token", has_next_token},
                    {"n_remain", n_remain},
                    {"num_tokens_predicted", num_tokens_predicted},
                    {"stopped_eos", stopped_eos},
                    {"stopped_word", stopped_word},
                    {"stopped_limit", stopped_limit},
                    {"stopping_word", stopping_word},
                });

    return token_with_probs;
  }

  std::vector<float> getEmbedding() {
    static const int n_embd = llama_n_embd(model);
    if (!params.embedding) {
      LOG_WARNING("embedding disabled",
                  {
                      {"params.embedding", params.embedding},
                  });
      return std::vector<float>(n_embd, 0.0f);
    }
    const float *data = llama_get_embeddings(ctx);
    std::vector<float> embedding(data, data + n_embd);
    return embedding;
  }
};

/* ... */

static json format_generation_settings(llama_server_context &llama) {
  const auto &sparams = llama.params.sampling_params;
  const auto eos_bias = sparams.logit_bias.find(llama_token_eos(llama.ctx));
  const bool ignore_eos = eos_bias != sparams.logit_bias.end() &&
                          eos_bias->second < 0.0f &&
                          std::isinf(eos_bias->second);

  return json{
      {"n_ctx", llama.n_ctx},
      {"model", llama.params.model_alias},
      {"seed", llama.params.seed},
      {"temp", sparams.temp},
      {"top_k", sparams.top_k},
      {"top_p", sparams.top_p},
      {"tfs_z", sparams.tfs_z},
      {"typical_p", sparams.typical_p},
      {"repeat_last_n", sparams.repeat_last_n},
      {"repeat_penalty", sparams.repeat_penalty},
      {"presence_penalty", sparams.presence_penalty},
      {"frequency_penalty", sparams.frequency_penalty},
      {"mirostat", sparams.mirostat},
      {"mirostat_tau", sparams.mirostat_tau},
      {"mirostat_eta", sparams.mirostat_eta},
      {"penalize_nl", sparams.penalize_nl},
      {"stop", llama.params.antiprompt},
      {"n_predict", llama.params.n_predict},
      {"n_keep", llama.params.n_keep},
      {"ignore_eos", ignore_eos},
      {"stream", llama.stream},
      {"logit_bias", sparams.logit_bias},
      {"n_probs", sparams.n_probs},
      {"grammar", llama.params.grammar},
  };
}

static json format_embedding_response(llama_server_context &llama) {
  return json{
      {"embedding", llama.getEmbedding()},
  };
}

static json format_timings(llama_server_context &llama) {
  const auto timings = llama_get_timings(llama.ctx);

  return json{
      {"prompt_n", timings.n_p_eval},
      {"prompt_ms", timings.t_p_eval_ms},
      {"prompt_per_token_ms", timings.t_p_eval_ms / timings.n_p_eval},
      {"prompt_per_second", 1e3 / timings.t_p_eval_ms * timings.n_p_eval},

      {"predicted_n", timings.n_eval},
      {"predicted_ms", timings.t_eval_ms},
      {"predicted_per_token_ms", timings.t_eval_ms / timings.n_eval},
      {"predicted_per_second", 1e3 / timings.t_eval_ms * timings.n_eval},
  };
}

static json
format_final_response(llama_server_context &llama, const std::string &content,
                      const std::vector<completion_token_output> &probs, int statusCode=200) {

  json res = json{
      {"content", content},
      {"stop", true},
      {"model", llama.params.model_alias},
      {"tokens_predicted", llama.num_tokens_predicted},
      {"tokens_evaluated", llama.num_prompt_tokens},
      {"generation_settings", format_generation_settings(llama)},
      {"prompt", llama.prompt},
      {"truncated", llama.truncated},
      {"stopped_eos", llama.stopped_eos},
      {"stopped_word", llama.stopped_word},
      {"stopped_limit", llama.stopped_limit},
      {"stopping_word", llama.stopping_word},
      {"tokens_cached", llama.n_past},
      {"timings", format_timings(llama)},
      {"status", statusCode}
  };

  if (llama.params.sampling_params.n_probs > 0) {
    res["completion_probabilities"] = probs_vector_to_json(llama.ctx, probs);
  }

  return res;
}

static json
format_partial_response(llama_server_context &llama, const std::string &content,
                        const std::vector<completion_token_output> &probs) {
  json res = json{
      {"content", content},
      {"stop", false},
  };

  if (llama.params.sampling_params.n_probs > 0) {
    res["completion_probabilities"] = probs_vector_to_json(llama.ctx, probs);
  }

  return res;
}

static json format_tokenizer_response(const std::vector<llama_token> &tokens) {
  return json{{"tokens", tokens}};
}

static json format_detokenized_response(std::string content) {
  return json{{"content", content}};
}

template <typename T>
static T json_value(const json &body, const std::string &key,
                    const T &default_value) {
  // Fallback null to default value
  return body.contains(key) && !body.at(key).is_null()
             ? body.value(key, default_value)
             : default_value;
}

static bool RPC_DEBUG = false;

static void parse_options_completion(const json &body,
                                     llama_server_context &llama) {
  gpt_params default_params;
  const auto &default_sparams = default_params.sampling_params;
  auto &sparams = llama.params.sampling_params;

  RPC_DEBUG = json_value(body, "__debug", false);

  llama.stream = json_value(body, "stream", false);
  llama.params.n_predict =
      json_value(body, "n_predict", default_params.n_predict);
  sparams.top_k = json_value(body, "top_k", default_sparams.top_k);
  sparams.top_p = json_value(body, "top_p", default_sparams.top_p);
  sparams.tfs_z = json_value(body, "tfs_z", default_sparams.tfs_z);
  sparams.typical_p = json_value(body, "typical_p", default_sparams.typical_p);
  sparams.repeat_last_n =
      json_value(body, "repeat_last_n", default_sparams.repeat_last_n);
  sparams.temp = json_value(body, "temperature", default_sparams.temp);
  sparams.repeat_penalty =
      json_value(body, "repeat_penalty", default_sparams.repeat_penalty);
  sparams.presence_penalty =
      json_value(body, "presence_penalty", default_sparams.presence_penalty);
  sparams.frequency_penalty =
      json_value(body, "frequency_penalty", default_sparams.frequency_penalty);
  sparams.mirostat = json_value(body, "mirostat", default_sparams.mirostat);
  sparams.mirostat_tau =
      json_value(body, "mirostat_tau", default_sparams.mirostat_tau);
  sparams.mirostat_eta =
      json_value(body, "mirostat_eta", default_sparams.mirostat_eta);
  sparams.penalize_nl =
      json_value(body, "penalize_nl", default_sparams.penalize_nl);
  llama.params.n_keep = json_value(body, "n_keep", default_params.n_keep);
  llama.params.seed = json_value(body, "seed", default_params.seed);
  llama.params.grammar = json_value(body, "grammar", default_params.grammar);
  sparams.n_probs = json_value(body, "n_probs", default_sparams.n_probs);

  if (body.count("prompt") != 0) {
    llama.prompt = body["prompt"];
  } else {
    llama.prompt = "";
  }

  sparams.logit_bias.clear();
  if (json_value(body, "ignore_eos", false)) {
    sparams.logit_bias[llama_token_eos(llama.ctx)] = -INFINITY;
  }

  const auto &logit_bias = body.find("logit_bias");
  if (logit_bias != body.end() && logit_bias->is_array()) {
    const int n_vocab = llama_n_vocab(llama.model);
    for (const auto &el : *logit_bias) {
      if (el.is_array() && el.size() == 2 && el[0].is_number_integer()) {
        llama_token tok = el[0].get<llama_token>();
        if (tok >= 0 && tok < n_vocab) {
          if (el[1].is_number()) {
            sparams.logit_bias[tok] = el[1].get<float>();
          } else if (el[1].is_boolean() && !el[1].get<bool>()) {
            sparams.logit_bias[tok] = -INFINITY;
          }
        }
      }
    }
  }

  llama.params.antiprompt.clear();
  const auto &stop = body.find("stop");
  // std::cout << "\n\n";
  // std::cout << "[LOG] Stop/antiprompt: ";
  
  if (stop != body.end() && stop->is_array()) {
    for (const auto &word : *stop) {
      if (!word.empty()) {
        llama.params.antiprompt.push_back(word);
      }
      // std::cout << "\"" << word << "\", ";
    }
    // std::cout << "\n\n";
  }

  llama.ctx_sampling = llama_sampling_context_init(llama.params, llama.grammar);

  LOG_VERBOSE("completion parameters parsed",
              format_generation_settings(llama));
}


static bool is_at_eob(llama_server_context &server_context,
                      const llama_token *tokens, const size_t n_tokens) {
  return n_tokens &&
         tokens[n_tokens - 1] == llama_token_eos(server_context.ctx);
}

// Function matching type llama_beam_search_callback_fn_t.
// Custom callback example is called each time the beams lengths increase:
//  * Show progress by printing ',' following by number of convergent beam
//  tokens if any.
//  * When all beams converge to a common prefix, they are made available in
//  beams_state.beams[0].
//    This is also called when the stop condition is met.
//    Collect tokens into std::vector<llama_token> response which is pointed to
//    by callback_data.
static void beam_search_callback(void *callback_data,
                                 llama_beams_state beams_state) {
  auto &llama = *static_cast<llama_server_context *>(callback_data);
  // Mark beams as EOS as needed.
  for (size_t i = 0; i < beams_state.n_beams; ++i) {
    llama_beam_view &beam_view = beams_state.beam_views[i];
    if (!beam_view.eob &&
        is_at_eob(llama, beam_view.tokens, beam_view.n_tokens)) {
      beam_view.eob = true;
    }
  }
  printf(","); // Show progress
  if (const size_t n = beams_state.common_prefix_length) {
    llama.generated_token_probs.resize(llama.generated_token_probs.size() + n);
    assert(0u < beams_state.n_beams);
    const llama_token *tokens = beams_state.beam_views[0].tokens;
    const auto map = [](llama_token tok) {
      return completion_token_output{{}, tok};
    };
    std::transform(tokens, tokens + n, llama.generated_token_probs.end() - n,
                   map);
    printf("%zu", n);
  }
  fflush(stdout);
#if 0 // DEBUG: print current beams for this iteration
    std::cout << "\n\nCurrent beams:\n";
    for (size_t i=0 ; i < beams_state.n_beams ; ++i) {
        std::cout << "beams["<<i<<"]: " << ostream_beam_view{state.ctx,beams_state.beam_views[i]} << std::endl;
    }
#endif
}

struct token_translator {
  llama_context *ctx;
  std::string operator()(llama_token tok) const {
    return llama_token_to_piece(ctx, tok);
  }
  std::string operator()(const completion_token_output &cto) const {
    return (*this)(cto.tok);
  }
};

static void append_to_generated_text_from_generated_token_probs(
    llama_server_context &llama) {
  auto &gtps = llama.generated_token_probs;
  auto translator = token_translator{llama.ctx};
  auto add_strlen = [=](size_t sum, const completion_token_output &cto) {
    return sum + translator(cto).size();
  };
  const size_t len =
      std::accumulate(gtps.begin(), gtps.end(), size_t(0), add_strlen);
  if (llama.generated_text.capacity() < llama.generated_text.size() + len) {
    llama.generated_text.reserve(llama.generated_text.size() + len);
  }
  for (const completion_token_output &cto : gtps) {
    llama.generated_text += translator(cto);
  }
}

void parse_args(const char* commandLine, int& argc, char**& argv) {
    const int kMaxArgs = 64;
    argv = new char*[kMaxArgs];
    argc = 0;

    char* p2 = strtok(const_cast<char*>(commandLine), " ");
    while (p2 && argc < kMaxArgs-1) {
        argv[argc++] = p2;
        p2 = strtok(0, " ");
    }
    argv[argc] = 0;
}

// static Server svr;
// struct that contains llama context and inference
static llama_server_context llama;

const char* tokenize(const char* req_json) {

  json req = json::parse(req_json);
  json ret = json({{"success", false}});
  
  if (req.find("text") != req.end() && req.at("text").is_string()) {
    auto prompt_tokens = llama.tokenize(req.at("text"), true); // always add BOS
    int num_prompt_tokens = prompt_tokens.size();
    ret["tokens"] = json::array();
    for (const auto& item : prompt_tokens) {
      ret["tokens"].push_back(llama_token_to_piece(llama.ctx, item));
    }
    ret["length"] = num_prompt_tokens;
    ret["success"] = true;
  }
  
  return strdup(ret.dump(-1, ' ', false, json::error_handler_t::replace).c_str());
}

const char* get_completion(const char* req_json) {

    auto lock = llama.lock();

    int res_status = 400;

    llama.rewind();

    llama_reset_timings(llama.ctx);

    parse_options_completion(json::parse(req_json), llama);

    if (RPC_DEBUG) {
      std::cout << "DEBUG COMPLETION INPUT: " << req_json << "\n";
    }

    if (!llama.loadGrammar()) {
      res_status = 400;
      return "{error}";
    }

    json prompt_stats = llama.loadPrompt();
    llama.beginCompletion();

    if (!llama.stream) {
      if (llama.params.n_beams) {
        // Fill llama.generated_token_probs vector with final beam.
        llama_beam_search(llama.ctx, beam_search_callback, &llama,
                          llama.params.n_beams, llama.n_past, llama.n_remain);
        // Translate llama.generated_token_probs to llama.generated_text.
        append_to_generated_text_from_generated_token_probs(llama);
      } else {
        size_t stop_pos = std::string::npos;

        while (llama.has_next_token) {
          const completion_token_output token_with_probs = llama.doCompletion();
          const std::string token_text =
              token_with_probs.tok == -1
                  ? ""
                  : llama_token_to_piece(llama.ctx, token_with_probs.tok);

          stop_pos = llama.findStoppingStrings(llama.generated_text,
                                               token_text.size(), STOP_FULL);
        }

        if (stop_pos == std::string::npos) {
          stop_pos =
              llama.findStoppingStrings(llama.generated_text, 0, STOP_PARTIAL);
        }
        if (stop_pos != std::string::npos) {
          llama.generated_text.erase(llama.generated_text.begin() + stop_pos,
                                     llama.generated_text.end());
        }
      }

      auto probs = llama.generated_token_probs;
      if (llama.params.sampling_params.n_probs > 0 && llama.stopped_word) {
        const std::vector<llama_token> stop_word_toks =
            llama_tokenize(llama.ctx, llama.stopping_word, false);
        probs = std::vector<completion_token_output>(
            llama.generated_token_probs.begin(),
            llama.generated_token_probs.end() - stop_word_toks.size());
      }

      res_status = 200;

      json data =
          format_final_response(llama, llama.generated_text, probs, res_status);

      data["prompt_stats"] = json(prompt_stats);

      auto *ret = new std::string(data.dump(-1, ' ', false, json::error_handler_t::replace));

      if (RPC_DEBUG) {
        std::cout << "DEBUG COMPLETION OUTPUT: " << *ret << "\n";
      }

      const char* ret_cstr = strdup(ret->c_str());

      if (RPC_DEBUG) {
        printf("DEBUG COMPLETION C_STR OUTPUT: %s \n", ret_cstr);
      }

      return ret_cstr;
    }

    return "{\"status\":500, \"error\":\"streaming not supported\"}";
}

std::mutex async_compl_mutex;
uint32_t async_compl_computation_status = 0;
bool async_compl_cancel_requested;
std::vector<json> async_compl_storage = {}; // = new std::vector<std::string*>();
std::thread* computation_thread;
std::unique_lock<std::mutex> async_compl_llama_lock;


void async_completion_worker() {
    
    llama.beginCompletion();

    // llama_print_timings(llama.ctx);

    // res.set_content(data.dump(-1, ' ', false, json::error_handler_t::replace),
    //                 "application/json");
    // } else {
    
    // const auto chunked_content_provider = [&](size_t, DataSink &sink) {
      size_t sent_count = 0;
      size_t sent_token_probs_index = 0;
  
      while (llama.has_next_token && async_compl_cancel_requested != 1) {
        const completion_token_output token_with_probs = llama.doCompletion();
        if (token_with_probs.tok == -1 || llama.multibyte_pending > 0) {
          continue;
        }
        const std::string token_text =
            llama_token_to_piece(llama.ctx, token_with_probs.tok);
  
        size_t pos = std::min(sent_count, llama.generated_text.size());
  
        const std::string str_test = llama.generated_text.substr(pos);
        bool is_stop_full = false;
        size_t stop_pos =
            llama.findStoppingStrings(str_test, token_text.size(), STOP_FULL);
        if (stop_pos != std::string::npos) {
          is_stop_full = true;
          llama.generated_text.erase(llama.generated_text.begin() + pos +
                                          stop_pos,
                                      llama.generated_text.end());
          pos = std::min(sent_count, llama.generated_text.size());
        } else {
          is_stop_full = false;
          stop_pos = llama.findStoppingStrings(str_test, token_text.size(),
                                                STOP_PARTIAL);
        }
  
        if (stop_pos == std::string::npos ||
            // Send rest of the text if we are at the end of the generation
            (!llama.has_next_token && !is_stop_full && stop_pos > 0)) {
          const std::string to_send =
              llama.generated_text.substr(pos, std::string::npos);
  
          sent_count += to_send.size();
  
          std::vector<completion_token_output> probs_output = {};
  
          if (llama.params.sampling_params.n_probs > 0) {
            const std::vector<llama_token> to_send_toks =
                llama_tokenize(llama.ctx, to_send, false);
            size_t probs_pos = std::min(sent_token_probs_index,
                                        llama.generated_token_probs.size());
            size_t probs_stop_pos =
                std::min(sent_token_probs_index + to_send_toks.size(),
                          llama.generated_token_probs.size());
            if (probs_pos < probs_stop_pos) {
              probs_output = std::vector<completion_token_output>(
                  llama.generated_token_probs.begin() + probs_pos,
                  llama.generated_token_probs.begin() + probs_stop_pos);
            }
            sent_token_probs_index = probs_stop_pos;
          }
  
          const json data =
              format_partial_response(llama, to_send, probs_output);
  
          // const std::string str =
          //     "data: " +
          //     data.dump(-1, ' ', false, json::error_handler_t::replace) +
          //     "\n\n";

          // const std::string str = data.dump(-1, ' ', false, json::error_handler_t::replace) 
  
          LOG_VERBOSE("data stream", {{"to_send", data.dump(-1, ' ', false, json::error_handler_t::replace)}});

          async_compl_mutex.lock();
          async_compl_storage.push_back(data);
          async_compl_mutex.unlock();
          
          // TODO: consumer failure case  
          // if (!sink.write(str.data(), str.size())) {
          //   LOG_VERBOSE("stream closed", {});
          //   llama_print_timings(llama.ctx);
          //   return false;
          // }
        }
  
        if (!llama.has_next_token) {
          // Generation is done, send extra information.
          const json data =
              format_final_response(llama, "",
                                    std::vector<completion_token_output>(
                                        llama.generated_token_probs.begin(),
                                        llama.generated_token_probs.begin() +
                                            sent_token_probs_index));
    
          LOG_VERBOSE("data stream", {{"to_send", data.dump(-1, ' ', false, json::error_handler_t::replace)}});

          async_compl_mutex.lock();
          async_compl_storage.push_back(data);
          async_compl_mutex.unlock();

          // if (!sink.write(str.data(), str.size())) {
          //   LOG_VERBOSE("stream closed", {});
          //   llama_print_timings(llama.ctx);
          //   return false;
          // }
        }
      }
  
      // llama_print_timings(llama.ctx);
    
    //sink.done();
    // return true;
    // };

    // const auto on_complete = [&](bool) { llama.mutex.unlock(); };

    async_compl_mutex.lock();

    async_compl_computation_status = 2;
    async_compl_llama_lock.release();
    llama.mutex.unlock();
    
    async_compl_mutex.unlock();


    // res.set_chunked_content_provider("text/event-stream",
    //                                  chunked_content_provider, on_complete);
    //}
}

const char* async_completion_init(const char* req_json) {
    async_compl_mutex.lock();
    async_compl_computation_status = 1;
    async_compl_cancel_requested = 0;

    // In case something from previous request remains
    // TODO free strings in async_compl_storage
    async_compl_storage.clear();

    async_compl_llama_lock = llama.lock();

    // int res_status = 400;

    llama.rewind();

    llama_reset_timings(llama.ctx);

    parse_options_completion(json::parse(req_json), llama);

    llama.stream = true;

    if (RPC_DEBUG) {
      std::cout << "DEBUG COMPLETION INPUT: " << req_json << "\n";
    }

    // if (!llama.loadGrammar()) {
    //   res_status = 400;
    //   return "{error}";
    // }

    json prompt_stats = llama.loadPrompt();

    async_compl_mutex.unlock();

    computation_thread = new std::thread(&async_completion_worker);
    computation_thread->detach();

    // TODO free
    json ret = json::object();
    ret["success"] = true;
    ret["prompt_stats"] = json(prompt_stats);
    return strdup(ret.dump(-1, ' ', false, json::error_handler_t::replace).c_str());
}

const char* async_completion_cancel(const char* req_json) {
  async_compl_mutex.lock();
  
  async_compl_cancel_requested = 1;

  // async_compl_computation_status = 0;
  
  // async_compl_llama_lock.unlock();
  // llama.rewind();
  // llama_reset_timings(llama.ctx);
  
  async_compl_mutex.unlock();
  json ret = json::object({{"success", true}});
  return strdup(ret.dump(-1, ' ', false, json::error_handler_t::replace).c_str());
}

// TODO: cancel functionality
const char* async_completion_poll(const char* cmd_json) {
  async_compl_mutex.lock();

  json ret = json::object({{"success", true}});
  ret["completion_updates"] = json::array();
  
  for (const auto& item : async_compl_storage) {
    ret["completion_updates"].push_back(item);
  }

  // TODO free
  async_compl_storage.clear();

  async_compl_mutex.unlock();

  // TODO free
  ret["success"] = true;

  // TODO more consistent feature
  if (async_compl_computation_status == 2) {
    ret["finished"] = true;
  } else {
    ret["finished"] = false;
  }

  return strdup(ret.dump(-1, ' ', false, json::error_handler_t::replace).c_str());
}

void from_json(const json& j, llama_sampling_params& p) { /* ...*/}

void from_json(const json& j, gpt_params& p) { /* ... */ }

void from_json(const json& j, server_params& p) {
    auto it = j.find("hostname");
    if (it != j.end()) {
        p.hostname = it.value().get<std::string>();
    }

    it = j.find("public_path");
    if (it != j.end()) {
        p.public_path = it.value().get<std::string>();
    }

    it = j.find("port");
    if (it != j.end()) {
        p.port = it.value().get<int32_t>();
    }

    it = j.find("read_timeout");
    if (it != j.end()) {
        p.read_timeout = it.value().get<int32_t>();
    }

    it = j.find("write_timeout");
    if (it != j.end()) {
        p.write_timeout = it.value().get<int32_t>();
    }
}

void server_params_parse_from_json(const char* json_string, gpt_params &sparams, server_params &params) {
  json j = json::parse(json_string);
  from_json(j, sparams);
  from_json(j, params);
}

int init(const char* cmd) {
  int argc;
  char** argv;

  // own arguments required by this example
  gpt_params params;
  server_params sparams;

  if (cmd[0] == '{') {
    server_params_parse_from_json(cmd, params, sparams);
  } else {
    parse_args(cmd, argc, argv);
    server_params_parse(argc, argv, sparams, params);
  }

  if (params.model_alias == "unknown") {
    params.model_alias = params.model;
  }

  llama_backend_init(params.numa);

  LOG_INFO("build info", {{"build", BUILD_NUMBER}, {"commit", BUILD_COMMIT}});
  LOG_INFO("system info",
           {
               {"n_threads", params.n_threads},
               {"n_threads_batch", params.n_threads_batch},
               {"total_threads", std::thread::hardware_concurrency()},
               {"system_info", llama_print_system_info()},
           });

  // load the model
  if (!llama.loadModel(params)) {
    llama.init_success = -1;
    return 1;
  }

  llama.init_success = 1;

  return 0;
}


void init_async(const char* cmd) {
  printf("INIT: %s", cmd);
  auto init_thread = new std::thread(init, cmd);
  init_thread->detach();
}

const char *poll_system_status() {
  json j;
  
  j["init_success"] = llama.init_success;
  j["model_alias"] = llama.params.model_alias;
  j["model"] = llama.params.model;

  return strdup(j.dump(-1, ' ', false, json::error_handler_t::replace).c_str());
}

void deinit() {
  llama_backend_free();
}
```

```C++
/* specultive decoding example for llama.cpp */
#include "build-info.h"

#include "common.h"
#include "llama.h"
#include "grammar-parser.h"

#include <cmath>
#include <cstdio>
#include <string>
#include <vector>

int main(int argc, char ** argv) {
    gpt_params params;

    if (gpt_params_parse(argc, argv, params) == false) {
        return 1;
    }

    if (params.model_draft.empty()) {
        fprintf(stderr, "%s: error: --model-draft is required\n", __func__);
        return 1;
    }

#ifndef LOG_DISABLE_LOGS
    log_set_target(log_filename_generator("speculative", "log"));
    LOG_TEE("Log start\n");
    log_dump_cmdline(argc, argv);
#endif // LOG_DISABLE_LOGS

    // init llama.cpp
    llama_backend_init(params.numa);

    llama_model * model_tgt = NULL;
    llama_model * model_dft = NULL;

    llama_context * ctx_tgt = NULL;
    llama_context * ctx_dft = NULL;

    // load the target model
    params.logits_all = true;
    std::tie(model_tgt, ctx_tgt) = llama_init_from_gpt_params(params);

    // load the draft model
    params.model = params.model_draft;
    params.n_gpu_layers = params.n_gpu_layers_draft;
    std::tie(model_dft, ctx_dft) = llama_init_from_gpt_params(params);

    // tokenize the prompt
    std::vector<llama_token> inp;
    inp = ::llama_tokenize(ctx_tgt, params.prompt, true);

    const int max_context_size     = llama_n_ctx(ctx_tgt);
    const int max_tokens_list_size = max_context_size - 4;

    if ((int) inp.size() > max_tokens_list_size) {
        fprintf(stderr, "%s: error: prompt too long (%d tokens, max %d)\n", __func__, (int) inp.size(), max_tokens_list_size);
        return 1;
    }

    fprintf(stderr, "\n\n");

    for (auto id : inp) {
        fprintf(stderr, "%s", llama_token_to_piece(ctx_tgt, id).c_str());
    }

    fflush(stderr);

    const int n_input = inp.size();

    const auto t_enc_start = ggml_time_us();

    // eval the prompt with both models
    llama_decode(ctx_tgt, llama_batch_get_one( inp.data(), n_input - 1, 0,           0));
    llama_decode(ctx_tgt, llama_batch_get_one(&inp.back(),           1, n_input - 1, 0));
    llama_decode(ctx_dft, llama_batch_get_one( inp.data(), n_input,     0,           0));

    const auto t_enc_end = ggml_time_us();

    // the 2 models should have the same vocab
    const int n_ctx   = llama_n_ctx(ctx_tgt);
    const int n_vocab = llama_n_vocab(model_tgt);
    //GGML_ASSERT(n_vocab == llama_n_vocab(model_dft));

    // how many tokens to draft each time
    int n_draft = params.n_draft;

    int n_predict = 0;
    int n_drafted = 0;
    int n_accept  = 0;

    int n_past_tgt = inp.size();
    int n_past_dft = inp.size();

    std::vector<llama_token> drafted;

    std::vector<llama_token> last_tokens(n_ctx);
    std::fill(last_tokens.begin(), last_tokens.end(), 0);

    for (auto & id : inp) {
        last_tokens.erase(last_tokens.begin());
        last_tokens.push_back(id);
    }

    std::vector<llama_token_data> candidates;
    candidates.reserve(n_vocab);

    // used to determine end of generation
    bool has_eos = false;

    // grammar stuff
    struct llama_grammar * grammar_dft = NULL;
    struct llama_grammar * grammar_tgt = NULL;

    grammar_parser::parse_state parsed_grammar;

    // if requested - load the grammar, error checking is omitted for brevity
    if (!params.grammar.empty()) {
        parsed_grammar = grammar_parser::parse(params.grammar.c_str());
        // will be empty (default) if there are parse errors
        if (parsed_grammar.rules.empty()) {
            return 1;
        }

        std::vector<const llama_grammar_element *> grammar_rules(parsed_grammar.c_rules());
        grammar_tgt = llama_grammar_init(grammar_rules.data(), grammar_rules.size(), parsed_grammar.symbol_ids.at("root"));
    }

    llama_sampling_context ctx_sampling = llama_sampling_context_init(params, grammar_tgt);

    const auto t_dec_start = ggml_time_us();

    while (true) {
        LOG("drafted: %s\n", LOG_TOKENS_TOSTR_PRETTY(ctx_dft, drafted));

        int i_dft = 0;

        while (true) {
            // sample from the target model
            llama_token id = llama_sampling_sample(ctx_tgt, NULL, ctx_sampling, last_tokens, candidates, i_dft);

            // remember which tokens were sampled - used for repetition penalties during sampling
            last_tokens.erase(last_tokens.begin());
            last_tokens.push_back(id);

            //LOG("last: %s\n", LOG_TOKENS_TOSTR_PRETTY(ctx_tgt, last_tokens));

            const std::string token_str = llama_token_to_piece(ctx_tgt, id);
            printf("%s", token_str.c_str());
            fflush(stdout);

            if (id == llama_token_eos(ctx_tgt)) {
                has_eos = true;
            }

            ++n_predict;

            // check if the draft matches the target
            if (i_dft < (int) drafted.size() && id == drafted[i_dft]) {
                LOG("the sampled target token matches the %dth drafted token (%d, '%s') - accepted\n", i_dft, id, token_str.c_str());
                ++n_accept;
                ++n_past_tgt;
                ++n_past_dft;
                ++i_dft;

                continue;
            }

            // the drafted token was rejected or we are out of drafted tokens

            if (i_dft < (int) drafted.size()) {
                LOG("the %dth drafted token (%d, '%s') does not match the sampled target token (%d, '%s') - rejected\n",
                        i_dft, drafted[i_dft], llama_token_to_piece(ctx_dft, drafted[i_dft]).c_str(), id, token_str.c_str());
            } else {
                LOG("out of drafted tokens\n");
            }

            llama_kv_cache_seq_rm(ctx_dft, 0, n_past_dft, -1);
            llama_decode(ctx_dft, llama_batch_get_one(&id, 1, n_past_dft, 0));
            ++n_past_dft;

            // heuristic for n_draft
            {
                const int  n_draft_cur  = (int) drafted.size();
                const bool all_accepted = i_dft == n_draft_cur;

                LOG("n_draft      = %d\n", n_draft);
                LOG("n_draft_cur  = %d\n", n_draft_cur);
                LOG("i_dft        = %d\n", i_dft);
                LOG("all_accepted = %d\n", all_accepted);

                if (all_accepted && n_draft == n_draft_cur) {
                    LOG(" - max drafted tokens accepted - n_draft += 8\n");
                    n_draft = std::min(30, n_draft + 8);
                } else if (all_accepted) {
                    LOG(" - partially drafted tokens accepted - no change\n");
                } else {
                    LOG(" - drafted token rejected - n_draft -= 1\n");
                    n_draft = std::max(2, n_draft - 1);
                }
            }

            drafted.clear();
            drafted.push_back(id);

            break;
        }

        if (n_predict > params.n_predict || has_eos) {
            break;
        }

        if (grammar_tgt) {
            if (grammar_dft) {
                llama_grammar_free(grammar_dft);
            }
            // Note: Hardcoded to sequence id 0, if this ever supports parallel generation
            //       that will need to change.
            auto it = ctx_sampling.sequence_contexts.find(0);
            GGML_ASSERT(it != ctx_sampling.sequence_contexts.end());
            // This is necessary because each sequence id in sequence_contexts
            // uses a copy of the original grammar.
            grammar_dft = llama_grammar_copy(it->second.grammar);

            LOG("copied target grammar to draft grammar\n");
        }

        // sample n_draft tokens from the draft model using greedy decoding
        int n_past_cur = n_past_dft;
        for (int i = 0; i < n_draft; ++i) {
            float * logits = llama_get_logits(ctx_dft);

            candidates.clear();
            for (llama_token token_id = 0; token_id < n_vocab; token_id++) {
                candidates.emplace_back(llama_token_data{token_id, logits[token_id], 0.0f});
            }

            llama_token_data_array cur_p = { candidates.data(), candidates.size(), false };

            if (grammar_dft != NULL) {
                llama_sample_grammar(ctx_dft, &cur_p, grammar_dft);
            }

            // computes softmax and sorts the candidates
            llama_sample_softmax(ctx_dft, &cur_p);

            for (int i = 0; i < 3; ++i) {
                LOG(" - draft candidate %3d: %6d (%8.3f) '%s'\n", i, cur_p.data[i].id, cur_p.data[i].p, llama_token_to_piece(ctx_dft, cur_p.data[i].id).c_str());
            }

            // TODO: better logic?
            if (cur_p.data[0].p < 2*cur_p.data[1].p) {
                LOG("stopping drafting, probability too low: %.3f < 2*%.3f\n", cur_p.data[0].p, cur_p.data[1].p);
                break;
            }

            // drafted token
            const llama_token id = cur_p.data[0].id;

            drafted.push_back(id);
            ++n_drafted;

            // no need to evaluate the last drafted token, since we won't use the result
            if (i == n_draft - 1) {
                break;
            }

            // evaluate the drafted token on the draft model
            llama_kv_cache_seq_rm(ctx_dft, 0, n_past_cur, -1);
            llama_decode(ctx_dft, llama_batch_get_one(&drafted.back(), 1, n_past_cur, 0));
            ++n_past_cur;

            if (grammar_dft != NULL) {
                llama_grammar_accept_token(ctx_dft, grammar_dft, id);
            }
        }

        // evaluate the target model on the drafted tokens
        llama_kv_cache_seq_rm(ctx_tgt, 0, n_past_tgt, -1);
        llama_decode(ctx_tgt, llama_batch_get_one(drafted.data(), drafted.size(), n_past_tgt, 0));
        ++n_past_tgt;

        // the first token is always proposed by the traget model before the speculation loop
        drafted.erase(drafted.begin());
    }

    auto t_dec_end = ggml_time_us();

    LOG_TEE("\n\n");

    LOG_TEE("encoded %4d tokens in %8.3f seconds, speed: %8.3f t/s\n", n_input,   (t_enc_end - t_enc_start) / 1e6f, inp.size() / ((t_enc_end - t_enc_start) / 1e6f));
    LOG_TEE("decoded %4d tokens in %8.3f seconds, speed: %8.3f t/s\n", n_predict, (t_dec_end - t_dec_start) / 1e6f, n_predict / ((t_dec_end - t_dec_start) / 1e6f));

    // TODO: make sure these numbers are computed correctly
    LOG_TEE("\n");
    LOG_TEE("n_draft   = %d\n", n_draft);
    LOG_TEE("n_predict = %d\n", n_predict);
    LOG_TEE("n_drafted = %d\n", n_drafted);
    LOG_TEE("n_accept  = %d\n", n_accept);
    LOG_TEE("accept    = %.3f%%\n", 100.0f * n_accept / n_drafted);

    LOG_TEE("\ndraft:\n");
    llama_print_timings(ctx_dft);

    LOG_TEE("\ntarget:\n");
    llama_print_timings(ctx_tgt);

    llama_free(ctx_tgt);
    llama_free_model(model_tgt);

    llama_free(ctx_dft);
    llama_free_model(model_dft);

    if (grammar_dft != NULL) {
        llama_grammar_free(grammar_dft);
        llama_grammar_free(grammar_tgt);
    }
    llama_backend_free();

    fprintf(stderr, "\n\n");

    return 0;
}
```