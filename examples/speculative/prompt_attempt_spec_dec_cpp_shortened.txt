As a professional C++ engineer, analyze the three source code files I give you. First two files 'rpcserver.h' 'rpcserver.cpp' introduce my own json rpc code for LLM sampling for llama.cpp, the other file 'speculative.cpp' is a speculative decoding example from the llama.cpp repo. Using speculative.cpp as a refernce and template I need you to analyze possible integration issues, plan out step by step, and write a speculative decoding (sampling) API integrated into my json rpc source code. Note that I had to omit some parts of librpcserver.cpp source code for brevity.

The API surface for speculative decoding I would like looks like this (ignore the sync sampling API as legacy):
```C++
void init_async(const char* cmd, bool init_draft_model); // extended implementation that allows to call it a second time to init a separate draft model
const char *poll_system_status(); // extended to output info about draft model status if present or loading
const char* async_completion_init(const char* req_json); // extended json option allows speculative sampling
/* ... */
```
Here is an informative blogpost on LLM speculative decoding with example in python for reference:

```
The article discusses DeepMind's paper on "Accelerating Large Language Model Decoding with Speculative Sampling". The authors propose a method called speculative sampling, which is a technique for decoding text from large language models more efficiently.

The standard method of generating text from a language model is through autoregressive sampling. The algorithm provided in the paper is implemented in the function `autoregressive_sampling(x, model, N)`. Here, `x` is a list of integers representing the token ids of the input text, `model` is a language model like GPT-2, and `N` is the number of tokens we want to decode. The time complexity of this algorithm is proportional to the number of tokens to decode and the time taken for a single forward pass of the model .

Speculative sampling, on the other hand, uses two models: a smaller, faster draft model and a larger, slower target model. The draft model speculates what the output is steps into the future, while the target model determines how many of those tokens we should accept. The algorithm involves decoding tokens in the regular autoregressive fashion, getting the probability outputs of the target and draft model on the new predicted sequence, and comparing the target and draft model probabilities to determine how many of the tokens we want to keep based on some rejection criteria. If a token is rejected, it is resampled using a combination of the two distributions and no more tokens are accepted. If all tokens are accepted, an additional final token is sampled from the target model probability output .

The full algorithm for speculative sampling is defined in the paper and implemented in the function `speculative_sampling(x, draft_model, target_model, N, K)`. The time complexity for this algorithm is proportional to the number of iterations in the while loop and the time complexity for each iteration in the loop .

# Python reference implementation:
def max_fn(x):
    x_max = np.where(x > 0, x, 0)
    return x_max / np.sum(x_max)

def speculative_sampling(x, draft_model, target_model, N, K):
    # NOTE: paper indexes arrays starting from 1, python indexes from 0, so
    # we have to add an extra -1 term when indexing using n, T, or t
    n = len(x)
    T = len(x) + N

    while n < T:
        # Step 1: auto-regressive decode K tokens from draft model and get final p
        x_draft = x
        for _ in range(K):
            p = draft_model(x_draft)
            x_draft = np.append(x_draft, sample(p[-1]))

        # Step 2: target model forward passes on x_draft
        q = target_model(x_draft)

        # Step 3: append draft tokens based on rejection criterion and resample
        # a token on rejection
        all_accepted = True
        for _ in range(K):
            i = n - 1
            j = x_draft[i + 1]
            if np.random.random() < min(1, q[i][j] / p[i][j]):  # accepted
                x = np.append(x, j)
                n += 1
            else:  # rejected
                x = np.append(x, sample(max_fn(q[i] - p[i])))  # resample
                n += 1
                all_accepted = False
                break

        # Step 4: if all draft tokens were accepted, sample a final token
        if all_accepted:
            x = np.append(x, sample(q[-1]))
            n += 1

        # just keeping my sanity
        assert n == len(x), f"{n} {len(x)}"

    return x

The authors report empirical speedup numbers for their 70B Chinchilla model using a specially trained 7B Chinchilla as the draft model. The decoding process is reported to be 2 times faster as compared to autoregressive decoding. The empirical speedup numbers were compared to theoretical speedup numbers, which were calculated using the time complexity equations provided in the paper .

In conclusion, speculative sampling is a technique that can be used with existing models alongside other inference techniques such as quantization, hardware acceleration, flash attention, etc. It requires no changes to the model's architecture, training, or anything like that. It can also be used with top-p/top-k/temperature.
```

```C++
/*  rpcserver.h json rpc for llama.cpp inference - my code */
#include <stdbool.h>
#include <stddef.h>
#include <stdint.h>

#ifdef __cplusplus
extern "C" {
#endif

int init(const char *cmd);

void init_async(const char* cmd);

const char* tokenize(const char* req_json);

const char *poll_system_status();

const char* get_completion(const char* req_json);

const char* async_completion_init(const char* req_json);

const char* async_completion_poll(const char* cmd_json);

const char* async_completion_cancel(const char* req_json);

void deinit();

#ifdef __cplusplus
}
#endif
```

```C++
/* rpcserver.cpp json rpc for llama.cpp inference - my code */
#include "build-info.h"
#include "common.h"
#include "grammar-parser.h"
#include "llama.h"
#include "rpcserver.h"

#ifndef NDEBUG
// crash the server in debug mode, otherwise send an http 500 error
#define CPPHTTPLIB_NO_EXCEPTIONS 1
#endif

// #include "httplib.h"
#include "json.hpp"

// auto generated files (update with ./deps.sh)
#include "completion.js.hpp"
#include "index.html.hpp"
#include "index.js.hpp"
#include "json-schema-to-grammar.mjs.hpp"

#include <cstddef>

#ifndef SERVER_VERBOSE
#define SERVER_VERBOSE 1
#endif

// using namespace httplib;
using json = nlohmann::json;

void parse_args(const char* commandLine, int& argc, char**& argv);

struct server_params {
  std::string hostname = "127.0.0.1";
  std::string public_path = "examples/server/public";
  int32_t port = 8080;
  int32_t read_timeout = 600;
  int32_t write_timeout = 600;
};

// completion token output with probabilities
struct completion_token_output {
  struct token_prob {
    llama_token tok;
    float prob;
  };

  std::vector<token_prob> probs;
  llama_token tok;
};

static size_t common_part(const std::vector<llama_token> &a,
                          const std::vector<llama_token> &b) {
  size_t i;
  for (i = 0; i < a.size() && i < b.size() && a[i] == b[i]; i++) {
  }
  return i;
}

enum stop_type {
  STOP_FULL,
  STOP_PARTIAL,
};

static bool ends_with(const std::string &str, const std::string &suffix) {
  return str.size() >= suffix.size() &&
         0 == str.compare(str.size() - suffix.size(), suffix.size(), suffix);
}

static size_t find_partial_stop_string(const std::string &stop,
                                       const std::string &text) {
  if (!text.empty() && !stop.empty()) {
    const char text_last_char = text.back();
    for (int64_t char_index = stop.size() - 1; char_index >= 0; char_index--) {
      if (stop[char_index] == text_last_char) {
        const std::string current_partial = stop.substr(0, char_index + 1);
        if (ends_with(text, current_partial)) {
          return text.size() - char_index - 1;
        }
      }
    }
  }
  return std::string::npos;
}

template <class Iter>
static std::string tokens_to_str(llama_context *ctx, Iter begin, Iter end) {
  std::string ret;
  for (; begin != end; ++begin) {
    ret += llama_token_to_piece(ctx, *begin);
  }
  return ret;
}

static void server_log(const char *level, const char *function, int line,
                       const char *message,
                       const nlohmann::ordered_json &extra) {
  nlohmann::ordered_json log{
      {"timestamp", time(nullptr)}, {"level", level},
      {"function", function},       {"line", line},
      {"message", message},
  };

  if (!extra.empty()) {
    log.merge_patch(extra);
  }

  const std::string str =
      log.dump(-1, ' ', false, json::error_handler_t::replace);
  printf("%.*s\n", (int)str.size(), str.data());
  fflush(stdout);
}

/* ... */

static bool server_verbose = false;

#if SERVER_VERBOSE != 1
#define LOG_VERBOSE(MSG, ...)
#else
#define LOG_VERBOSE(MSG, ...)                                                  \
  do {                                                                         \
    if (server_verbose) {                                                      \
      server_log("VERBOSE", __func__, __LINE__, MSG, __VA_ARGS__);             \
    }                                                                          \
  } while (0)
#endif

#define LOG_ERROR(MSG, ...)                                                    \
  server_log("ERROR", __func__, __LINE__, MSG, __VA_ARGS__)
#define LOG_WARNING(MSG, ...)                                                  \
  server_log("WARNING", __func__, __LINE__, MSG, __VA_ARGS__)
#define LOG_INFO(MSG, ...)                                                     \
  server_log("INFO", __func__, __LINE__, MSG, __VA_ARGS__)

/* ... */

static void append_to_generated_text_from_generated_token_probs(
    llama_server_context &llama) {
  auto &gtps = llama.generated_token_probs;
  auto translator = token_translator{llama.ctx};
  auto add_strlen = [=](size_t sum, const completion_token_output &cto) {
    return sum + translator(cto).size();
  };
  const size_t len =
      std::accumulate(gtps.begin(), gtps.end(), size_t(0), add_strlen);
  if (llama.generated_text.capacity() < llama.generated_text.size() + len) {
    llama.generated_text.reserve(llama.generated_text.size() + len);
  }
  for (const completion_token_output &cto : gtps) {
    llama.generated_text += translator(cto);
  }
}

void parse_args(const char* commandLine, int& argc, char**& argv) {
 /* ... */
}

// static Server svr;
// struct that contains llama context and inference
static llama_server_context llama;

const char* tokenize(const char* req_json) {

  json req = json::parse(req_json);
  json ret = json({{"success", false}});
  
  if (req.find("text") != req.end() && req.at("text").is_string()) {
    auto prompt_tokens = llama.tokenize(req.at("text"), true); // always add BOS
    int num_prompt_tokens = prompt_tokens.size();
    ret["tokens"] = json::array();
    for (const auto& item : prompt_tokens) {
      ret["tokens"].push_back(llama_token_to_piece(llama.ctx, item));
    }
    ret["length"] = num_prompt_tokens;
    ret["success"] = true;
  }
  
  return strdup(ret.dump(-1, ' ', false, json::error_handler_t::replace).c_str());
}

const char* get_completion(const char* req_json) {

    auto lock = llama.lock();

    int res_status = 400;

    llama.rewind();

    llama_reset_timings(llama.ctx);

    parse_options_completion(json::parse(req_json), llama);

    if (RPC_DEBUG) {
      std::cout << "DEBUG COMPLETION INPUT: " << req_json << "\n";
    }

    if (!llama.loadGrammar()) {
      res_status = 400;
      return "{error}";
    }

    json prompt_stats = llama.loadPrompt();
    llama.beginCompletion();

    if (!llama.stream) {
      if (llama.params.n_beams) {
        // Fill llama.generated_token_probs vector with final beam.
        llama_beam_search(llama.ctx, beam_search_callback, &llama,
                          llama.params.n_beams, llama.n_past, llama.n_remain);
        // Translate llama.generated_token_probs to llama.generated_text.
        append_to_generated_text_from_generated_token_probs(llama);
      } else {
        size_t stop_pos = std::string::npos;

        while (llama.has_next_token) {
          const completion_token_output token_with_probs = llama.doCompletion();
          const std::string token_text =
              token_with_probs.tok == -1
                  ? ""
                  : llama_token_to_piece(llama.ctx, token_with_probs.tok);

          stop_pos = llama.findStoppingStrings(llama.generated_text,
                                               token_text.size(), STOP_FULL);
        }

        if (stop_pos == std::string::npos) {
          stop_pos =
              llama.findStoppingStrings(llama.generated_text, 0, STOP_PARTIAL);
        }
        if (stop_pos != std::string::npos) {
          llama.generated_text.erase(llama.generated_text.begin() + stop_pos,
                                     llama.generated_text.end());
        }
      }

      auto probs = llama.generated_token_probs;
      if (llama.params.sampling_params.n_probs > 0 && llama.stopped_word) {
        const std::vector<llama_token> stop_word_toks =
            llama_tokenize(llama.ctx, llama.stopping_word, false);
        probs = std::vector<completion_token_output>(
            llama.generated_token_probs.begin(),
            llama.generated_token_probs.end() - stop_word_toks.size());
      }

      res_status = 200;

      json data =
          format_final_response(llama, llama.generated_text, probs, res_status);

      data["prompt_stats"] = json(prompt_stats);

      auto *ret = new std::string(data.dump(-1, ' ', false, json::error_handler_t::replace));

      if (RPC_DEBUG) {
        std::cout << "DEBUG COMPLETION OUTPUT: " << *ret << "\n";
      }

      const char* ret_cstr = strdup(ret->c_str());

      if (RPC_DEBUG) {
        printf("DEBUG COMPLETION C_STR OUTPUT: %s \n", ret_cstr);
      }

      return ret_cstr;
    }

    return "{\"status\":500, \"error\":\"streaming not supported\"}";
}

std::mutex async_compl_mutex;
uint32_t async_compl_computation_status = 0;
bool async_compl_cancel_requested;
std::vector<json> async_compl_storage = {}; // = new std::vector<std::string*>();
std::thread* computation_thread;
std::unique_lock<std::mutex> async_compl_llama_lock;


void async_completion_worker() {
    
    llama.beginCompletion();

    // llama_print_timings(llama.ctx);

    // res.set_content(data.dump(-1, ' ', false, json::error_handler_t::replace),
    //                 "application/json");
    // } else {
    
    // const auto chunked_content_provider = [&](size_t, DataSink &sink) {
      size_t sent_count = 0;
      size_t sent_token_probs_index = 0;
  
      while (llama.has_next_token && async_compl_cancel_requested != 1) {
        const completion_token_output token_with_probs = llama.doCompletion();
        if (token_with_probs.tok == -1 || llama.multibyte_pending > 0) {
          continue;
        }
        const std::string token_text =
            llama_token_to_piece(llama.ctx, token_with_probs.tok);
  
        size_t pos = std::min(sent_count, llama.generated_text.size());
  
        const std::string str_test = llama.generated_text.substr(pos);
        bool is_stop_full = false;
        size_t stop_pos =
            llama.findStoppingStrings(str_test, token_text.size(), STOP_FULL);
        if (stop_pos != std::string::npos) {
          is_stop_full = true;
          llama.generated_text.erase(llama.generated_text.begin() + pos +
                                          stop_pos,
                                      llama.generated_text.end());
          pos = std::min(sent_count, llama.generated_text.size());
        } else {
          is_stop_full = false;
          stop_pos = llama.findStoppingStrings(str_test, token_text.size(),
                                                STOP_PARTIAL);
        }
  
        if (stop_pos == std::string::npos ||
            // Send rest of the text if we are at the end of the generation
            (!llama.has_next_token && !is_stop_full && stop_pos > 0)) {
          const std::string to_send =
              llama.generated_text.substr(pos, std::string::npos);
  
          sent_count += to_send.size();
  
          std::vector<completion_token_output> probs_output = {};
  
          if (llama.params.sampling_params.n_probs > 0) {
            const std::vector<llama_token> to_send_toks =
                llama_tokenize(llama.ctx, to_send, false);
            size_t probs_pos = std::min(sent_token_probs_index,
                                        llama.generated_token_probs.size());
            size_t probs_stop_pos =
                std::min(sent_token_probs_index + to_send_toks.size(),
                          llama.generated_token_probs.size());
            if (probs_pos < probs_stop_pos) {
              probs_output = std::vector<completion_token_output>(
                  llama.generated_token_probs.begin() + probs_pos,
                  llama.generated_token_probs.begin() + probs_stop_pos);
            }
            sent_token_probs_index = probs_stop_pos;
          }
  
          const json data =
              format_partial_response(llama, to_send, probs_output);

          LOG_VERBOSE("data stream", {{"to_send", data.dump(-1, ' ', false, json::error_handler_t::replace)}});

          async_compl_mutex.lock();
          async_compl_storage.push_back(data);
          async_compl_mutex.unlock();

        }
  
        if (!llama.has_next_token) {
          // Generation is done, send extra information.
          const json data =
              format_final_response(llama, "",
                                    std::vector<completion_token_output>(
                                        llama.generated_token_probs.begin(),
                                        llama.generated_token_probs.begin() +
                                            sent_token_probs_index));
    
          LOG_VERBOSE("data stream", {{"to_send", data.dump(-1, ' ', false, json::error_handler_t::replace)}});

          async_compl_mutex.lock();
          async_compl_storage.push_back(data);
          async_compl_mutex.unlock();

        }
      }
  
    async_compl_mutex.lock();

    async_compl_computation_status = 2;
    async_compl_llama_lock.release();
    llama.mutex.unlock();
    
    async_compl_mutex.unlock();
}

const char* async_completion_init(const char* req_json) {
    async_compl_mutex.lock();
    async_compl_computation_status = 1;
    async_compl_cancel_requested = 0;

    // In case something from previous request remains
    // TODO free strings in async_compl_storage
    async_compl_storage.clear();

    async_compl_llama_lock = llama.lock();

    // int res_status = 400;

    llama.rewind();

    llama_reset_timings(llama.ctx);

    parse_options_completion(json::parse(req_json), llama);

    llama.stream = true;

    if (RPC_DEBUG) {
      std::cout << "DEBUG COMPLETION INPUT: " << req_json << "\n";
    }

    json prompt_stats = llama.loadPrompt();

    async_compl_mutex.unlock();

    computation_thread = new std::thread(&async_completion_worker);
    computation_thread->detach();

    // TODO free
    json ret = json::object();
    ret["success"] = true;
    ret["prompt_stats"] = json(prompt_stats);
    return strdup(ret.dump(-1, ' ', false, json::error_handler_t::replace).c_str());
}

const char* async_completion_cancel(const char* req_json) {
  async_compl_mutex.lock();
  
  async_compl_cancel_requested = 1;

  async_compl_mutex.unlock();
  json ret = json::object({{"success", true}});
  return strdup(ret.dump(-1, ' ', false, json::error_handler_t::replace).c_str());
}

// TODO: cancel functionality
const char* async_completion_poll(const char* cmd_json) {
  async_compl_mutex.lock();

  json ret = json::object({{"success", true}});
  ret["completion_updates"] = json::array();
  
  for (const auto& item : async_compl_storage) {
    ret["completion_updates"].push_back(item);
  }

  // TODO free
  async_compl_storage.clear();

  async_compl_mutex.unlock();

  // TODO free
  ret["success"] = true;

  // TODO more consistent feature
  if (async_compl_computation_status == 2) {
    ret["finished"] = true;
  } else {
    ret["finished"] = false;
  }

  return strdup(ret.dump(-1, ' ', false, json::error_handler_t::replace).c_str());
}

void from_json(const json& j, llama_sampling_params& p) { /* ...*/}

void from_json(const json& j, gpt_params& p) { /* ... */ }

void from_json(const json& j, server_params& p) {
    auto it = j.find("hostname");
    if (it != j.end()) {
        p.hostname = it.value().get<std::string>();
    }

    it = j.find("public_path");
    if (it != j.end()) {
        p.public_path = it.value().get<std::string>();
    }

    it = j.find("port");
    if (it != j.end()) {
        p.port = it.value().get<int32_t>();
    }

    it = j.find("read_timeout");
    if (it != j.end()) {
        p.read_timeout = it.value().get<int32_t>();
    }

    it = j.find("write_timeout");
    if (it != j.end()) {
        p.write_timeout = it.value().get<int32_t>();
    }
}

void server_params_parse_from_json(const char* json_string, gpt_params &sparams, server_params &params) {
  json j = json::parse(json_string);
  from_json(j, sparams);
  from_json(j, params);
}

int init(const char* cmd) {
  int argc;
  char** argv;

  // own arguments required by this example
  gpt_params params;
  server_params sparams;

  if (cmd[0] == '{') {
    server_params_parse_from_json(cmd, params, sparams);
  } else {
    parse_args(cmd, argc, argv);
    server_params_parse(argc, argv, sparams, params);
  }

  if (params.model_alias == "unknown") {
    params.model_alias = params.model;
  }

  llama_backend_init(params.numa);

  LOG_INFO("build info", {{"build", BUILD_NUMBER}, {"commit", BUILD_COMMIT}});
  LOG_INFO("system info",
           {
               {"n_threads", params.n_threads},
               {"n_threads_batch", params.n_threads_batch},
               {"total_threads", std::thread::hardware_concurrency()},
               {"system_info", llama_print_system_info()},
           });

  // load the model
  if (!llama.loadModel(params)) {
    llama.init_success = -1;
    return 1;
  }

  llama.init_success = 1;

  return 0;
}


void init_async(const char* cmd) {
  printf("INIT: %s", cmd);
  auto init_thread = new std::thread(init, cmd);
  init_thread->detach();
}

const char *poll_system_status() {
  json j;
  
  j["init_success"] = llama.init_success;
  j["model_alias"] = llama.params.model_alias;
  j["model"] = llama.params.model;

  return strdup(j.dump(-1, ' ', false, json::error_handler_t::replace).c_str());
}

void deinit() {
  llama_backend_free();
}
```

```C++
/* specultive decoding example for llama.cpp */
#include "build-info.h"

#include "common.h"
#include "llama.h"
#include "grammar-parser.h"

#include <cmath>
#include <cstdio>
#include <string>
#include <vector>

int main(int argc, char ** argv) {
    gpt_params params;

    if (gpt_params_parse(argc, argv, params) == false) {
        return 1;
    }

    if (params.model_draft.empty()) {
        fprintf(stderr, "%s: error: --model-draft is required\n", __func__);
        return 1;
    }

#ifndef LOG_DISABLE_LOGS
    log_set_target(log_filename_generator("speculative", "log"));
    LOG_TEE("Log start\n");
    log_dump_cmdline(argc, argv);
#endif // LOG_DISABLE_LOGS

    // init llama.cpp
    llama_backend_init(params.numa);

    llama_model * model_tgt = NULL;
    llama_model * model_dft = NULL;

    llama_context * ctx_tgt = NULL;
    llama_context * ctx_dft = NULL;

    // load the target model
    params.logits_all = true;
    std::tie(model_tgt, ctx_tgt) = llama_init_from_gpt_params(params);

    // load the draft model
    params.model = params.model_draft;
    params.n_gpu_layers = params.n_gpu_layers_draft;
    std::tie(model_dft, ctx_dft) = llama_init_from_gpt_params(params);

    // tokenize the prompt
    std::vector<llama_token> inp;
    inp = ::llama_tokenize(ctx_tgt, params.prompt, true);

    const int max_context_size     = llama_n_ctx(ctx_tgt);
    const int max_tokens_list_size = max_context_size - 4;

    if ((int) inp.size() > max_tokens_list_size) {
        fprintf(stderr, "%s: error: prompt too long (%d tokens, max %d)\n", __func__, (int) inp.size(), max_tokens_list_size);
        return 1;
    }

    fprintf(stderr, "\n\n");

    for (auto id : inp) {
        fprintf(stderr, "%s", llama_token_to_piece(ctx_tgt, id).c_str());
    }

    fflush(stderr);

    const int n_input = inp.size();

    const auto t_enc_start = ggml_time_us();

    // eval the prompt with both models
    llama_decode(ctx_tgt, llama_batch_get_one( inp.data(), n_input - 1, 0,           0));
    llama_decode(ctx_tgt, llama_batch_get_one(&inp.back(),           1, n_input - 1, 0));
    llama_decode(ctx_dft, llama_batch_get_one( inp.data(), n_input,     0,           0));

    const auto t_enc_end = ggml_time_us();

    // the 2 models should have the same vocab
    const int n_ctx   = llama_n_ctx(ctx_tgt);
    const int n_vocab = llama_n_vocab(model_tgt);
    //GGML_ASSERT(n_vocab == llama_n_vocab(model_dft));

    // how many tokens to draft each time
    int n_draft = params.n_draft;

    int n_predict = 0;
    int n_drafted = 0;
    int n_accept  = 0;

    int n_past_tgt = inp.size();
    int n_past_dft = inp.size();

    std::vector<llama_token> drafted;

    std::vector<llama_token> last_tokens(n_ctx);
    std::fill(last_tokens.begin(), last_tokens.end(), 0);

    for (auto & id : inp) {
        last_tokens.erase(last_tokens.begin());
        last_tokens.push_back(id);
    }

    std::vector<llama_token_data> candidates;
    candidates.reserve(n_vocab);

    // used to determine end of generation
    bool has_eos = false;

    // grammar stuff
    struct llama_grammar * grammar_dft = NULL;
    struct llama_grammar * grammar_tgt = NULL;

    grammar_parser::parse_state parsed_grammar;

    // if requested - load the grammar, error checking is omitted for brevity
    if (!params.grammar.empty()) {
        parsed_grammar = grammar_parser::parse(params.grammar.c_str());
        // will be empty (default) if there are parse errors
        if (parsed_grammar.rules.empty()) {
            return 1;
        }

        std::vector<const llama_grammar_element *> grammar_rules(parsed_grammar.c_rules());
        grammar_tgt = llama_grammar_init(grammar_rules.data(), grammar_rules.size(), parsed_grammar.symbol_ids.at("root"));
    }

    llama_sampling_context ctx_sampling = llama_sampling_context_init(params, grammar_tgt);

    const auto t_dec_start = ggml_time_us();

    while (true) {
        LOG("drafted: %s\n", LOG_TOKENS_TOSTR_PRETTY(ctx_dft, drafted));

        int i_dft = 0;

        while (true) {
            // sample from the target model
            llama_token id = llama_sampling_sample(ctx_tgt, NULL, ctx_sampling, last_tokens, candidates, i_dft);

            // remember which tokens were sampled - used for repetition penalties during sampling
            last_tokens.erase(last_tokens.begin());
            last_tokens.push_back(id);

            //LOG("last: %s\n", LOG_TOKENS_TOSTR_PRETTY(ctx_tgt, last_tokens));

            const std::string token_str = llama_token_to_piece(ctx_tgt, id);
            printf("%s", token_str.c_str());
            fflush(stdout);

            if (id == llama_token_eos(ctx_tgt)) {
                has_eos = true;
            }

            ++n_predict;

            // check if the draft matches the target
            if (i_dft < (int) drafted.size() && id == drafted[i_dft]) {
                LOG("the sampled target token matches the %dth drafted token (%d, '%s') - accepted\n", i_dft, id, token_str.c_str());
                ++n_accept;
                ++n_past_tgt;
                ++n_past_dft;
                ++i_dft;

                continue;
            }

            // the drafted token was rejected or we are out of drafted tokens

            if (i_dft < (int) drafted.size()) {
                LOG("the %dth drafted token (%d, '%s') does not match the sampled target token (%d, '%s') - rejected\n",
                        i_dft, drafted[i_dft], llama_token_to_piece(ctx_dft, drafted[i_dft]).c_str(), id, token_str.c_str());
            } else {
                LOG("out of drafted tokens\n");
            }

            llama_kv_cache_seq_rm(ctx_dft, 0, n_past_dft, -1);
            llama_decode(ctx_dft, llama_batch_get_one(&id, 1, n_past_dft, 0));
            ++n_past_dft;

            // heuristic for n_draft
            {
                const int  n_draft_cur  = (int) drafted.size();
                const bool all_accepted = i_dft == n_draft_cur;

                LOG("n_draft      = %d\n", n_draft);
                LOG("n_draft_cur  = %d\n", n_draft_cur);
                LOG("i_dft        = %d\n", i_dft);
                LOG("all_accepted = %d\n", all_accepted);

                if (all_accepted && n_draft == n_draft_cur) {
                    LOG(" - max drafted tokens accepted - n_draft += 8\n");
                    n_draft = std::min(30, n_draft + 8);
                } else if (all_accepted) {
                    LOG(" - partially drafted tokens accepted - no change\n");
                } else {
                    LOG(" - drafted token rejected - n_draft -= 1\n");
                    n_draft = std::max(2, n_draft - 1);
                }
            }

            drafted.clear();
            drafted.push_back(id);

            break;
        }

        if (n_predict > params.n_predict || has_eos) {
            break;
        }

        if (grammar_tgt) {
            if (grammar_dft) {
                llama_grammar_free(grammar_dft);
            }
            // Note: Hardcoded to sequence id 0, if this ever supports parallel generation
            //       that will need to change.
            auto it = ctx_sampling.sequence_contexts.find(0);
            GGML_ASSERT(it != ctx_sampling.sequence_contexts.end());
            // This is necessary because each sequence id in sequence_contexts
            // uses a copy of the original grammar.
            grammar_dft = llama_grammar_copy(it->second.grammar);

            LOG("copied target grammar to draft grammar\n");
        }

        // sample n_draft tokens from the draft model using greedy decoding
        int n_past_cur = n_past_dft;
        for (int i = 0; i < n_draft; ++i) {
            float * logits = llama_get_logits(ctx_dft);

            candidates.clear();
            for (llama_token token_id = 0; token_id < n_vocab; token_id++) {
                candidates.emplace_back(llama_token_data{token_id, logits[token_id], 0.0f});
            }

            llama_token_data_array cur_p = { candidates.data(), candidates.size(), false };

            if (grammar_dft != NULL) {
                llama_sample_grammar(ctx_dft, &cur_p, grammar_dft);
            }

            // computes softmax and sorts the candidates
            llama_sample_softmax(ctx_dft, &cur_p);

            for (int i = 0; i < 3; ++i) {
                LOG(" - draft candidate %3d: %6d (%8.3f) '%s'\n", i, cur_p.data[i].id, cur_p.data[i].p, llama_token_to_piece(ctx_dft, cur_p.data[i].id).c_str());
            }

            // TODO: better logic?
            if (cur_p.data[0].p < 2*cur_p.data[1].p) {
                LOG("stopping drafting, probability too low: %.3f < 2*%.3f\n", cur_p.data[0].p, cur_p.data[1].p);
                break;
            }

            // drafted token
            const llama_token id = cur_p.data[0].id;

            drafted.push_back(id);
            ++n_drafted;

            // no need to evaluate the last drafted token, since we won't use the result
            if (i == n_draft - 1) {
                break;
            }

            // evaluate the drafted token on the draft model
            llama_kv_cache_seq_rm(ctx_dft, 0, n_past_cur, -1);
            llama_decode(ctx_dft, llama_batch_get_one(&drafted.back(), 1, n_past_cur, 0));
            ++n_past_cur;

            if (grammar_dft != NULL) {
                llama_grammar_accept_token(ctx_dft, grammar_dft, id);
            }
        }

        // evaluate the target model on the drafted tokens
        llama_kv_cache_seq_rm(ctx_tgt, 0, n_past_tgt, -1);
        llama_decode(ctx_tgt, llama_batch_get_one(drafted.data(), drafted.size(), n_past_tgt, 0));
        ++n_past_tgt;

        // the first token is always proposed by the traget model before the speculation loop
        drafted.erase(drafted.begin());
    }

    auto t_dec_end = ggml_time_us();

    LOG_TEE("\n\n");

    LOG_TEE("encoded %4d tokens in %8.3f seconds, speed: %8.3f t/s\n", n_input,   (t_enc_end - t_enc_start) / 1e6f, inp.size() / ((t_enc_end - t_enc_start) / 1e6f));
    LOG_TEE("decoded %4d tokens in %8.3f seconds, speed: %8.3f t/s\n", n_predict, (t_dec_end - t_dec_start) / 1e6f, n_predict / ((t_dec_end - t_dec_start) / 1e6f));

    // TODO: make sure these numbers are computed correctly
    LOG_TEE("\n");
    LOG_TEE("n_draft   = %d\n", n_draft);
    LOG_TEE("n_predict = %d\n", n_predict);
    LOG_TEE("n_drafted = %d\n", n_drafted);
    LOG_TEE("n_accept  = %d\n", n_accept);
    LOG_TEE("accept    = %.3f%%\n", 100.0f * n_accept / n_drafted);

    LOG_TEE("\ndraft:\n");
    llama_print_timings(ctx_dft);

    LOG_TEE("\ntarget:\n");
    llama_print_timings(ctx_tgt);

    llama_free(ctx_tgt);
    llama_free_model(model_tgt);

    llama_free(ctx_dft);
    llama_free_model(model_dft);

    if (grammar_dft != NULL) {
        llama_grammar_free(grammar_dft);
        llama_grammar_free(grammar_tgt);
    }
    llama_backend_free();

    fprintf(stderr, "\n\n");

    return 0;
}
```